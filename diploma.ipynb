{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErmakovSemen/diploma/blob/main/diploma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu3-DHVnjgzM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Classes\n"
      ],
      "metadata": {
        "id": "ZQiML9g-8AXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoBoHUmWjiT5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import binom\n",
        "\n",
        "class Assessor:\n",
        "    def __init__(self, id, confusion_matrix):\n",
        "        self.id = id\n",
        "        self.confusion_matrix = confusion_matrix.copy()\n",
        "        self.history = []\n",
        "\n",
        "    def rate(self, true_label):\n",
        "        c = self.confusion_matrix\n",
        "        if true_label == 1:\n",
        "            p = c['TP'] / (c['TP'] + c['FN'])\n",
        "        else:\n",
        "            p = c['FP'] / (c['FP'] + c['TN'])\n",
        "        return int(np.random.rand() < p)\n",
        "\n",
        "    def p_truth_given_response(self, response, prior=0.5):\n",
        "        c = self.confusion_matrix\n",
        "        TP, FP, TN, FN = c['TP'], c['FP'], c['TN'], c['FN']\n",
        "        tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        if response == 1:\n",
        "            numer = tpr * prior\n",
        "            denom = tpr * prior + fpr * (1 - prior)\n",
        "        else:\n",
        "            numer = (1 - tpr) * prior\n",
        "            denom = (1 - tpr) * prior + (1 - fpr) * (1 - prior)\n",
        "        return numer / denom if denom > 0 else prior\n",
        "\n",
        "    def theory_metrics(self):\n",
        "        c = self.confusion_matrix\n",
        "        TP, FP, TN, FN = c['TP'], c['FP'], c['TN'], c['FN']\n",
        "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        return {'precision': precision, 'recall': recall, 'f1': f1, 'fpr': fpr}\n",
        "\n",
        "    @staticmethod\n",
        "    def confusion_matrix_from_f1(target_f1, pr_coeff=1):\n",
        "        recall = target_f1 * (pr_coeff + 1) / (2 * pr_coeff)\n",
        "        precision = pr_coeff * recall\n",
        "        if not (0 < recall <= 1 and 0 < precision <= 1):\n",
        "            return None\n",
        "        TP = recall\n",
        "        FN = 1 - recall\n",
        "        FP = TP / precision - TP if precision > 0 else 1\n",
        "        TN = 1 - FP\n",
        "        if FP < 0 or FP > 1 or TN < 0 or TN > 1:\n",
        "            return None\n",
        "        return {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN}\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_noisy_confusion_matrix_from_f1(target_f1, pr_coeff=1, noise_std=0.01, max_tries=5):\n",
        "        for _ in range(max_tries):\n",
        "            base = Assessor.confusion_matrix_from_f1(target_f1, pr_coeff=pr_coeff)\n",
        "            if base is None: continue\n",
        "            cmatrix = {\n",
        "                k: np.clip(base[k] + np.random.normal(0, noise_std), 0.01, 0.99)\n",
        "                for k in base\n",
        "            }\n",
        "            # Нормировка\n",
        "            pos_sum = cmatrix['TP'] + cmatrix['FN']\n",
        "            neg_sum = cmatrix['FP'] + cmatrix['TN']\n",
        "            if pos_sum <= 0 or neg_sum <= 0: continue\n",
        "            cmatrix['TP'] /= pos_sum\n",
        "            cmatrix['FN'] /= pos_sum\n",
        "            cmatrix['FP'] /= neg_sum\n",
        "            cmatrix['TN'] /= neg_sum\n",
        "            if all(0 < v < 1 for v in cmatrix.values()):\n",
        "                return cmatrix\n",
        "        return None\n",
        "\n",
        "    @classmethod\n",
        "    def generate_diverse_assessors(cls, n, target_f1, pr_coeff=1, noise_std=0.01):\n",
        "        assessors = []\n",
        "        tries = 0\n",
        "        max_total_tries = 10 * n\n",
        "        while len(assessors) < n and tries < max_total_tries:\n",
        "            cmat = cls.generate_noisy_confusion_matrix_from_f1(target_f1, pr_coeff, noise_std)\n",
        "            tries += 1\n",
        "            if cmat is not None:\n",
        "                assessors.append(cls(len(assessors), cmat))\n",
        "        if len(assessors) < n:\n",
        "            print(f\"[WARN] Only {len(assessors)} of {n} assessors generated.\")\n",
        "        mean_emp_f1 = np.mean([a.theory_metrics()['f1'] for a in assessors]) if assessors else 0.\n",
        "        return assessors, mean_emp_f1\n",
        "\n",
        "\n",
        "# --- DataSimulator ---\n",
        "\n",
        "class DataSimulator:\n",
        "    def __init__(self, config, assessors=None):\n",
        "        self.config = config\n",
        "        self.assessors = assessors if assessors is not None else self._create_assessors()\n",
        "\n",
        "    def _create_assessors(self):\n",
        "        p = self.config[\"assessors\"]\n",
        "        # Допускаем только diverse_assessors-by-F1 способ\n",
        "        if 'target_f1' in p and 'pr_coeff' in p:\n",
        "            assessors = Assessor.generate_diverse_assessors(\n",
        "                p[\"total_count\"],\n",
        "                target_f1=p[\"target_f1\"],\n",
        "                pr_coeff=p[\"pr_coeff\"],\n",
        "                noise_std=p.get(\"noise\", 0.01)\n",
        "            )[0]\n",
        "            # print(assessors)\n",
        "            return assessors\n",
        "        elif 'matrix' in p:\n",
        "            # Для \"фиксированной\" толпы без разброса\n",
        "            n = p[\"total_count\"]\n",
        "            cmatrix = p[\"matrix\"]\n",
        "            return [Assessor(i, cmatrix) for i in range(n)]\n",
        "        else:\n",
        "            raise ValueError(\"Assessor config must contain 'matrix' или target_f1 & pr_coeff\")\n",
        "\n",
        "    def simulate_binary_tasks(self, debug=False):\n",
        "        group_probs = {\n",
        "            \"A\": self.config[\"experiment\"][\"prevalence\"],\n",
        "            \"B\": self.config[\"experiment\"][\"prevalence\"] + self.config[\"experiment\"][\"effect_size\"]\n",
        "        }\n",
        "        n_tasks = self.config[\"experiment\"][\"sample_size\"]\n",
        "        n_assessors = self.config[\"assessors\"][\"per_task_count\"]\n",
        "        tasks_by_group = {'A': [], 'B': []}\n",
        "        n_a = n_tasks // 2\n",
        "        n_b = n_tasks - n_a\n",
        "        for group, n in zip(['A', 'B'], [n_a, n_b]):\n",
        "            for _ in range(n):\n",
        "                p_true = group_probs[group]\n",
        "                true_label = int(np.random.rand() < p_true)\n",
        "                chosen = np.random.choice(self.assessors, size=n_assessors, replace=False)\n",
        "                responses = [(a.rate(true_label), a.id) for a in chosen]\n",
        "                tasks_by_group[group].append(responses)\n",
        "        assessors_by_id = {a.id: a for a in self.assessors}\n",
        "        if debug:\n",
        "            a_means = [np.mean([resp for resp, aid in t]) for t in tasks_by_group['A']]\n",
        "            b_means = [np.mean([resp for resp, aid in t]) for t in tasks_by_group['B']]\n",
        "            print(f\"[DataSimulator] mean(A)={np.mean(a_means):.3f}, mean(B)={np.mean(b_means):.3f}, diff={np.mean(b_means) - np.mean(a_means):.3f}\")\n",
        "        return tasks_by_group, assessors_by_id\n",
        "\n",
        "\n",
        "def poisson_binomial_variance(ps):\n",
        "    N = len(ps)\n",
        "    return np.sum([p * (1 - p) for p in ps]) / N**2\n",
        "\n",
        "def get_ps_for_label(assessors, true_label):\n",
        "    ps = []\n",
        "    for a in assessors:\n",
        "        c = a.confusion_matrix\n",
        "        if true_label == 1:\n",
        "            p = c['TP'] / (c['TP'] + c['FN'])\n",
        "        else:\n",
        "            p = c['FP'] / (c['FP'] + c['TN'])\n",
        "        ps.append(p)\n",
        "    return ps\n",
        "\n",
        "def pooled_poibin_variance(assessors, prevalence=0.5):\n",
        "    var1 = poisson_binomial_variance(get_ps_for_label(assessors, true_label=1))\n",
        "    var0 = poisson_binomial_variance(get_ps_for_label(assessors, true_label=0))\n",
        "    return prevalence * var1 + (1 - prevalence) * var0\n",
        "\n",
        "def pooled_poibin_variance(assessors, prevalence=0.5):\n",
        "    if isinstance(assessors, tuple):\n",
        "        assessors = assessors[0]\n",
        "    elif isinstance(assessors, list) and len(assessors) > 0 and isinstance(assessors[0], list):\n",
        "        assessors = [a for group in assessors for a in group]\n",
        "    var1 = poisson_binomial_variance([a.theory_metrics()['recall'] for a in assessors])\n",
        "    var0 = poisson_binomial_variance([a.theory_metrics()['fpr'] for a in assessors])\n",
        "    return prevalence * var1 + (1 - prevalence) * var0\n",
        "\n",
        "\n",
        "\n",
        "class AssessorProbabilisticAggregator:\n",
        "    @staticmethod\n",
        "    def aggregate_task(responses, assessors_by_id, prior=0.5, debug=False):\n",
        "        P_truth = []\n",
        "        for resp, aid in responses:\n",
        "            a = assessors_by_id[aid]\n",
        "            p = a.p_truth_given_response(resp, prior)\n",
        "            P_truth.append(p)\n",
        "        mean_p = np.mean(P_truth) if P_truth else 0.0\n",
        "        var_p = np.mean([p*(1-p) for p in P_truth]) / (len(P_truth) if len(P_truth) else 1)\n",
        "        if debug:\n",
        "            print(f\"[ProbAgg] mean_p={mean_p:.3f}, var_p={var_p:.3f}, P_truth={P_truth}\")\n",
        "        return mean_p, var_p\n",
        "\n",
        "\n",
        "\n",
        "def poisson_binomial_majority_variance(ps):\n",
        "    \"\"\"\n",
        "    Вернёт variance для majority vote на задаче (Bernoulli Poisson-binomial).\n",
        "    \"\"\"\n",
        "    N = len(ps)\n",
        "    pmf = np.zeros(N + 1)\n",
        "    pmf[0] = 1.0\n",
        "    for p in ps:\n",
        "        pmf[1:] = pmf[1:] * (1 - p) + pmf[:-1] * p\n",
        "        pmf[0] *= (1 - p)\n",
        "    threshold = N // 2 + 1\n",
        "    q_maj = pmf[threshold:].sum()\n",
        "    return q_maj * (1 - q_maj)\n",
        "\n",
        "def majority_var(ps):\n",
        "    N = len(ps)\n",
        "    pmf = np.zeros(N + 1)\n",
        "    pmf[0] = 1.0\n",
        "    for p in ps:\n",
        "        pmf[1:] = pmf[1:] * (1 - p) + pmf[:-1] * p\n",
        "        pmf[0] *= (1 - p)\n",
        "    threshold = N // 2 + 1\n",
        "    q_maj = pmf[threshold:].sum()\n",
        "    return q_maj * (1 - q_maj)\n",
        "\n",
        "def get_ps(task, assessors_by_id, is_pos):\n",
        "    return [\n",
        "        (assessors_by_id[aid].theory_metrics()['recall'] if is_pos else assessors_by_id[aid].theory_metrics()['fpr'])\n",
        "        for _, aid in task\n",
        "    ]\n",
        "\n",
        "\n",
        "class VarianceEstimator:\n",
        "    @staticmethod\n",
        "    def classic(tasks_by_group, *a, **k):\n",
        "        group_stats = {}\n",
        "        for group in ['A', 'B']:\n",
        "            majority = [\n",
        "                int(np.sum([label for label, _ in task]) >= (len(task) // 2 + 1))\n",
        "                for task in tasks_by_group[group]\n",
        "            ]\n",
        "            group_stats[group] = {'mean': np.mean(majority), 'var': np.var(majority, ddof=1)}\n",
        "        return {\n",
        "            'var_a': group_stats['A']['var'],\n",
        "            'var_b': group_stats['B']['var'],\n",
        "            'mean_a': group_stats['A']['mean'],\n",
        "            'mean_b': group_stats['B']['mean']\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def probabilistic(tasks_by_group, assessors_by_id, prior=0.5):\n",
        "        group_stats = {}\n",
        "        for group in ['A', 'B']:\n",
        "            # Mean aggregation per task (probabilistic mean)\n",
        "            means = [\n",
        "                np.mean([label for label, _ in task])  # simple arithmetic mean over all answers\n",
        "                for task in tasks_by_group[group]\n",
        "            ]\n",
        "            group_stats[group] = {'mean': np.mean(means), 'var': np.var(means, ddof=1)}\n",
        "        return {\n",
        "            'var_a': group_stats['A']['var'],\n",
        "            'var_b': group_stats['B']['var'],\n",
        "            'mean_a': group_stats['A']['mean'],\n",
        "            'mean_b': group_stats['B']['mean']\n",
        "        }\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def probabilistic_pooled(tasks_by_group, assessors_by_id, prior=0.5):\n",
        "        group_stats = {}\n",
        "        for group in ['A', 'B']:\n",
        "            ps_list = []\n",
        "            for task in tasks_by_group[group]:\n",
        "                ps = get_ps(task, assessors_by_id, True if prior >= 0.5 else False)\n",
        "                ps_list.extend(ps)\n",
        "            var = majority_var(ps_list)\n",
        "            mean_major_list = [\n",
        "                int(np.sum([label for label, _ in task]) >= (len(task) // 2 + 1))\n",
        "                for task in tasks_by_group[group]\n",
        "            ]\n",
        "            mean_group = np.mean(mean_major_list)\n",
        "            group_stats[group] = {'mean': mean_group, 'var': var}\n",
        "        return {\n",
        "            'var_a': group_stats['A']['var'],\n",
        "            'var_b': group_stats['B']['var'],\n",
        "            'mean_a': group_stats['A']['mean'],\n",
        "            'mean_b': group_stats['B']['mean']\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def probabilistic_pooled_taskwise(tasks_by_group, assessors_by_id, prior=0.5):\n",
        "        group_stats = {}\n",
        "        for group in ['A', 'B']:\n",
        "            var_list = []\n",
        "            for task in tasks_by_group[group]:\n",
        "                ps = get_ps(task, assessors_by_id, True if prior >= 0.5 else False)\n",
        "                var = majority_var(ps)\n",
        "                var_list.append(var)\n",
        "            var_group = np.mean(var_list)\n",
        "            mean_major_list = [\n",
        "                int(np.sum([label for label, _ in task]) >= (len(task) // 2 + 1))\n",
        "                for task in tasks_by_group[group]\n",
        "            ]\n",
        "            mean_group = np.mean(mean_major_list)\n",
        "            group_stats[group] = {'mean': mean_group, 'var': var_group}\n",
        "        return {\n",
        "            'var_a': group_stats['A']['var'],\n",
        "            'var_b': group_stats['B']['var'],\n",
        "            'mean_a': group_stats['A']['mean'],\n",
        "            'mean_b': group_stats['B']['mean']\n",
        "        }\n",
        "\n",
        "def single_simulation(config, prior=0.5, alpha=None, methods=None, assessors_override=None, debug=False):\n",
        "    if alpha is None:\n",
        "        alpha = config[\"experiment\"].get(\"alpha\", 0.05)\n",
        "    if methods is None:\n",
        "        methods = {\n",
        "            \"classic\": VarianceEstimator.classic,\n",
        "            \"probabilistic\": VarianceEstimator.probabilistic,\n",
        "            \"probabilistic_pooled\": VarianceEstimator.probabilistic_pooled,\n",
        "            \"probabilistic_pooled_taskwise\": VarianceEstimator.probabilistic_pooled_taskwise\n",
        "        }\n",
        "    has_effect = np.random.rand() > 0.5\n",
        "    effect_size = config[\"experiment\"][\"effect_size\"] if has_effect else 0.0\n",
        "    local_config = {**config, \"experiment\": {**config[\"experiment\"], \"effect_size\": effect_size}}\n",
        "    simulator = DataSimulator(local_config, assessors=assessors_override)\n",
        "    tasks_by_group, assessors_by_id = simulator.simulate_binary_tasks(debug=debug)\n",
        "    n_a, n_b = len(tasks_by_group['A']), len(tasks_by_group['B'])\n",
        "    if n_a < 2 or n_b < 2:\n",
        "        return []\n",
        "    res_list = []\n",
        "    for method_name, method in methods.items():\n",
        "        try:\n",
        "            res = method(tasks_by_group, assessors_by_id, prior)\n",
        "            var_ztest = res['var_a'] / n_a + res['var_b'] / n_b\n",
        "            se = np.sqrt(var_ztest)\n",
        "            effect = res['mean_b'] - res['mean_a']\n",
        "            p_value = 1.0 if se == 0 and effect == 0 else 0.0 if se == 0 else 2 * (1 - norm.cdf(abs(effect/se)))\n",
        "            out = {\n",
        "                \"method\": method_name,\n",
        "                \"p_value\": p_value,\n",
        "                \"effect_detected\": (p_value < alpha if p_value is not None else None),\n",
        "                \"true_effect\": has_effect,\n",
        "                \"mean_diff\": effect,\n",
        "                \"var_a\": res['var_a'],\n",
        "                \"var_b\": res['var_b'],\n",
        "                \"var_ztest\": var_ztest,\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "            res_list.append(out)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Метод {method_name} упал! Ошибка: {e}\")\n",
        "            if debug: print(f\"[{method_name}] failed: {e}\")\n",
        "            res_list.append({\n",
        "                \"method\": method_name, \"p_value\": None, \"effect_detected\": None, \"true_effect\": has_effect,\n",
        "                \"mean_diff\": None, \"var_a\": None, \"var_b\": None, \"var_ztest\": None, \"status\": f\"failed: {e}\"\n",
        "            })\n",
        "    return res_list\n",
        "\n",
        "class Plotter:\n",
        "    @staticmethod\n",
        "    def plot_f1_vs_param(df, param_name='mean_f1_assessor', methods=None, title=None):\n",
        "        if methods is None:\n",
        "            methods = df['method'].unique()\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        for method in methods:\n",
        "            part = df[df['method'] == method]\n",
        "            plt.plot(part[param_name], part['f1_test'], '-o', label=method.capitalize())\n",
        "        plt.xlabel(param_name)\n",
        "        plt.ylabel('F1-score теста')\n",
        "        if title:\n",
        "            plt.title(title)\n",
        "        else:\n",
        "            plt.title(f'F1-score теста vs {param_name}')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_f1_from_summary(summary, title=\"F1-score теста по методу\"):\n",
        "        if not summary:\n",
        "            print(\"Нет данных для графика summary.\")\n",
        "            return\n",
        "        methods = list(summary.keys())\n",
        "        xvals = methods\n",
        "        yvals = [summary[m]['f1_test'] for m in methods]\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.bar(xvals, yvals)\n",
        "        plt.ylabel('F1-score теста')\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_summary_vs_param(df, param_name='mean_f1_assessor', methods=None):\n",
        "        if methods is None:\n",
        "            methods = df['method'].unique()\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(9, 5))\n",
        "        for method in methods:\n",
        "            part = df[df['method'] == method]\n",
        "            ax.plot(part[param_name], part['f1_test'], '-o', label=f'{method.capitalize()} F1')\n",
        "            # Можно добавить ещё power/mean_effect, если решила считать\n",
        "            # ax.plot(part[param_name], part['power'], '--', label=f'{method.capitalize()} Power')\n",
        "            # ax.plot(part[param_name], part['mean_effect'], ':', label=f'{method.capitalize()} Effect')\n",
        "        ax.set_xlabel(param_name)\n",
        "        ax.set_ylabel('Метрика')\n",
        "        ax.set_title(f'Метрики теста vs {param_name}')\n",
        "        ax.legend()\n",
        "        ax.grid()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_pvalue_histogram(pvals, methods=None, bins=20):\n",
        "        plt.figure(figsize=(8,5))\n",
        "        if methods is None:\n",
        "            methods = list(pvals.keys())\n",
        "        for method in methods:\n",
        "            plt.hist(pvals[method], bins=np.linspace(0, 1, bins+1),\n",
        "                     alpha=0.5, density=True, label=method.capitalize())\n",
        "        plt.axhline(1, color='black', linestyle='--', label='Uniform')\n",
        "        plt.xlabel('p-value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Гистограмма p-value')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def simulate_fraction_zeros_mv(assessors, prevalence=0.5, n_experiments=500, n_tasks_per_exp=100):\n",
        "    N = len(assessors)\n",
        "    results = []\n",
        "    for _ in range(n_experiments):\n",
        "        zeros = 0\n",
        "        for _ in range(n_tasks_per_exp):\n",
        "            true_label = int(np.random.rand() < prevalence)\n",
        "            responses = np.array([a.rate(true_label) for a in assessors])\n",
        "            maj = int(np.sum(responses) >= (N // 2 + 1))\n",
        "            if maj == 0:\n",
        "                zeros += 1\n",
        "        results.append(zeros / n_tasks_per_exp)\n",
        "    return np.array(results)\n",
        "\n",
        "\n",
        "# -- Теоретическая вероятность majority==0 (по theory_metrics assessora)\n",
        "def poisson_binomial_pmf(ps):\n",
        "    N = len(ps)\n",
        "    pmf = np.zeros(N + 1)\n",
        "    pmf[0] = 1.0\n",
        "    for p in ps:\n",
        "        pmf[1:] = pmf[1:] * (1 - p) + pmf[:-1] * p\n",
        "        pmf[0] *= (1 - p)\n",
        "    return pmf\n",
        "\n",
        "def theoretical_prob_majority_zero(assessors, prevalence=0.5):\n",
        "    ps_1 = [a.theory_metrics()['recall'] for a in assessors]\n",
        "    ps_0 = [a.theory_metrics()['fpr'] for a in assessors]\n",
        "    N = len(ps_1)\n",
        "    thresh = N // 2 + 1\n",
        "    pmf1 = poisson_binomial_pmf(ps_1)\n",
        "    pmf0 = poisson_binomial_pmf(ps_0)\n",
        "    q1 = np.sum(pmf1[:thresh])  # P(majority==0|истина=1)\n",
        "    q0 = np.sum(pmf0[:thresh])  # P(majority==0|истина=0)\n",
        "    return prevalence * q1 + (1 - prevalence) * q0\n",
        "\n",
        "\n",
        "def aggregate_majority_vote_experiments(assessors, true_label=1, n_experiments=1000, n_trials_per_exp=100, agg_func='majority'):\n",
        "    \"\"\"\n",
        "    Для assessors/true_label моделирует n_experiments раз,\n",
        "    каждый раз делая n_trials_per_exp агрегаций голосования.\n",
        "    Возвращает: массив средних результатов по эксп. (или долей 0).\n",
        "    \"\"\"\n",
        "    N = len(assessors)\n",
        "    experiment_means = []\n",
        "    for _ in range(n_experiments):\n",
        "        votes = []\n",
        "        for _ in range(n_trials_per_exp):\n",
        "            responses = np.array([a.rate(true_label) for a in assessors])\n",
        "            if agg_func == 'majority':\n",
        "                value = int(np.sum(responses) >= (N // 2 + 1))\n",
        "            elif agg_func == 'mean':\n",
        "                value = np.mean(responses)\n",
        "            votes.append(value)\n",
        "        experiment_means.append(np.mean(votes))\n",
        "    return np.array(experiment_means)\n",
        "\n",
        "\n",
        "def expected_mean(assessors, true_label):\n",
        "    ps = []\n",
        "    for a in assessors:\n",
        "        c = a.confusion_matrix\n",
        "        if true_label == 1:\n",
        "            p = c['TP'] / (c['TP'] + c['FN'])\n",
        "        else:\n",
        "            p = c['FP'] / (c['FP'] + c['TN'])\n",
        "        ps.append(p)\n",
        "    return np.mean(ps)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Logic\n"
      ],
      "metadata": {
        "id": "ATY1C7ib8Me2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDsALAgOMS57"
      },
      "outputs": [],
      "source": [
        "def single_simulation(\n",
        "    config, prior=0.5, alpha=None, methods=None, assessors_override=None, debug=False\n",
        "):\n",
        "    if alpha is None:\n",
        "        alpha = config[\"experiment\"].get(\"alpha\", 0.05)\n",
        "    if methods is None:\n",
        "        methods = {\n",
        "            \"classic\": VarianceEstimator.classic,\n",
        "            \"probabilistic\": VarianceEstimator.probabilistic,\n",
        "            \"probabilistic_pooled\": VarianceEstimator.probabilistic_pooled,\n",
        "            \"probabilistic_pooled_taskwise\": VarianceEstimator.probabilistic_pooled_taskwise\n",
        "        }\n",
        "    has_effect = np.random.rand() > 0.5\n",
        "    effect_size = config[\"experiment\"][\"effect_size\"] if has_effect else 0.0\n",
        "    local_config = {**config, \"experiment\": {**config[\"experiment\"], \"effect_size\": effect_size}}\n",
        "    simulator = DataSimulator(local_config, assessors=assessors_override)\n",
        "    tasks_by_group, assessors_by_id = simulator.simulate_binary_tasks(debug=debug)\n",
        "    n_a, n_b = len(tasks_by_group['A']), len(tasks_by_group['B'])\n",
        "    if n_a < 2 or n_b < 2: return []\n",
        "    res_list = []\n",
        "    for method_name, method in methods.items():\n",
        "        try:\n",
        "            res = method(tasks_by_group, assessors_by_id, prior)\n",
        "            var_ztest = res['var_a'] / n_a + res['var_b'] / n_b\n",
        "            se = np.sqrt(var_ztest) if var_ztest >= 0 and np.isfinite(var_ztest) else 0.0\n",
        "            effect = res['mean_b'] - res['mean_a']\n",
        "            p_value = 1.0 if se == 0 and effect == 0 else 0.0 if se == 0 else 2 * (1 - norm.cdf(abs(effect/se)))\n",
        "            out = {\n",
        "                \"method\": method_name,\n",
        "                \"p_value\": p_value,\n",
        "                \"effect_detected\": (p_value < alpha if p_value is not None else None),\n",
        "                \"true_effect\": has_effect,\n",
        "                \"mean_diff\": effect,\n",
        "                \"var_a\": res['var_a'],\n",
        "                \"var_b\": res['var_b'],\n",
        "                \"var_ztest\": var_ztest,\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "\n",
        "            if debug:\n",
        "                print(f\"\\n[DEBUG SINGLE_SIMULATION] method: {method_name}\")\n",
        "                print(f\"  mean_a: {res['mean_a']:.4f}  mean_b: {res['mean_b']:.4f}  mean_diff: {effect:.4f}\")\n",
        "                print(f\"  var_a: {res['var_a']:.5f}  var_b: {res['var_b']:.5f}  var_ztest: {var_ztest:.5f}\")\n",
        "                print(f\"  se: {se:.5f}  p_value: {p_value:.5f}\")\n",
        "\n",
        "                # For 'probabilistic_pooled_taskwise', show variance per задаче:\n",
        "                if method_name == \"probabilistic_pooled_taskwise\":\n",
        "                    for group in ['A', 'B']:\n",
        "                        print(f\" |   {group} pooled-taskwise var: {res.get(f'var_{group[0]}', float('nan')):.6f}\")\n",
        "\n",
        "                # Majority outcome distribution (per A-группу/задачу) для диагностики\n",
        "                if method_name.startswith('prob'):\n",
        "                    majors_a = [\n",
        "                        int(np.sum([label for label, _ in task]) >= (len(task) // 2 + 1))\n",
        "                        for task in tasks_by_group['A']\n",
        "                    ]\n",
        "                    majors_b = [\n",
        "                        int(np.sum([label for label, _ in task]) >= (len(task) // 2 + 1))\n",
        "                        for task in tasks_by_group['B']\n",
        "                    ]\n",
        "                    print(f\"  majority vote (A): mean={np.mean(majors_a):.4f}, variance={np.var(majors_a):.5f}\")\n",
        "                    print(f\"  majority vote (B): mean={np.mean(majors_b):.4f}, variance={np.var(majors_b):.5f}\")\n",
        "\n",
        "            res_list.append(out)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Метод {method_name} упал! Ошибка: {e}\")\n",
        "            if debug: print(f\"[{method_name}] failed: {e}\")\n",
        "            res_list.append({\n",
        "                \"method\": method_name, \"p_value\": None, \"effect_detected\": None, \"true_effect\": has_effect,\n",
        "                \"mean_diff\": None, \"var_a\": None, \"var_b\": None, \"var_ztest\": None, \"status\": f\"failed: {e}\"\n",
        "            })\n",
        "    return res_list\n",
        "\n",
        "def run_exp(\n",
        "    config, n_simulations=100, prior=0.5, alpha=None,\n",
        "    methods=None, assessors_override=None, debug=False, plot=False, plot_param=None\n",
        "):\n",
        "\n",
        "    if methods is None:\n",
        "        methods = {\n",
        "            \"classic\": VarianceEstimator.classic,\n",
        "            \"probabilistic\": VarianceEstimator.probabilistic,\n",
        "            \"probabilistic_pooled\": VarianceEstimator.probabilistic_pooled,\n",
        "            \"probabilistic_pooled_taskwise\": VarianceEstimator.probabilistic_pooled_taskwise,\n",
        "        }\n",
        "    all_results = []\n",
        "    for sim in range(n_simulations):\n",
        "        sim_results = single_simulation(\n",
        "            config, prior=prior, alpha=alpha,\n",
        "            methods=methods, assessors_override=assessors_override, debug=debug\n",
        "        )\n",
        "        for result in sim_results:\n",
        "            result['simulation'] = sim\n",
        "            if 'method' not in result:\n",
        "                result['method'] = 'classic'\n",
        "            all_results.append(result)\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "    summary = {}\n",
        "    for method in df_results['method'].unique():\n",
        "        mdata = df_results[df_results['method'] == method]\n",
        "        good = mdata[mdata['status'] == 'success']\n",
        "        if len(good) < 1:\n",
        "            summary[method] = {k: None for k in ['power', 'fpr', 'success_rate', 'mean_effect', 'f1_test']}\n",
        "            continue\n",
        "        has_true = (good['true_effect'] == True).sum() > 0\n",
        "        has_false = (good['true_effect'] == False).sum() > 0\n",
        "        power = good[good['true_effect'] == True]['effect_detected'].mean() if has_true else None\n",
        "        fpr = good[good['true_effect'] == False]['effect_detected'].mean() if has_false else None\n",
        "        y_true = good['true_effect'].astype(int).values\n",
        "        y_pred = good['effect_detected'].fillna(False).astype(bool).astype(int).values\n",
        "        f1_t = f1_score(y_true, y_pred, zero_division=0) if len(np.unique(y_true)) > 1 and len(np.unique(y_pred)) > 1 else 0.0\n",
        "        summary[method] = {\n",
        "            \"power\": power,\n",
        "            \"fpr\": fpr,\n",
        "            \"success_rate\": len(good) / n_simulations,\n",
        "            \"mean_effect\": good['mean_diff'].mean() if good['mean_diff'].notna().any() else None,\n",
        "            \"f1_test\": f1_t\n",
        "        }\n",
        "    if plot and summary:\n",
        "        Plotter.plot_f1_from_summary_plotly(summary)\n",
        "    return df_results, summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def universal_sweep(\n",
        "    param_grid, param_name,\n",
        "    n_assessors=30, n_simulations=30,\n",
        "    base_config=None, sweep_type='f1', pr_coeff=1,\n",
        "    debug=False, plot=False, methods=None, metrics=('f1_test', 'power', 'fpr', 'variance', 'mean_effect')\n",
        "):\n",
        "    import pandas as pd\n",
        "    all_records = []\n",
        "    for param_val, ass_kwargs in tqdm(param_grid, desc=f'Sweeping {param_name}'):\n",
        "        assessors, _ = Assessor.generate_diverse_assessors(\n",
        "            n_assessors,\n",
        "            target_f1=ass_kwargs.get('target_f1', param_val),\n",
        "            pr_coeff=ass_kwargs.get('pr_coeff', pr_coeff),\n",
        "            noise_std=ass_kwargs.get('noise_std', 0.01)\n",
        "        )\n",
        "        if len(assessors) < n_assessors // 2:\n",
        "            if debug:\n",
        "                print(f\"[WARN] Only {len(assessors)} of {n_assessors} assessors generated at {param_name}={param_val}\")\n",
        "            continue\n",
        "        metrics_arr = [a.theory_metrics() for a in assessors]\n",
        "        mean_f1 = np.mean([m['f1'] for m in metrics_arr])\n",
        "        mean_prec = np.mean([m['precision'] for m in metrics_arr])\n",
        "        mean_rec = np.mean([m['recall'] for m in metrics_arr])\n",
        "        mean_fpr = np.mean([m['fpr'] for m in metrics_arr])\n",
        "        config = base_config.copy()\n",
        "        config[\"assessors\"][\"total_count\"] = n_assessors\n",
        "        config[\"assessors\"][\"per_task_count\"] = ass_kwargs.get('per_task_count', 5)\n",
        "        df_results, summary = run_exp(\n",
        "            config, n_simulations=n_simulations,\n",
        "            assessors_override=assessors, methods=methods, plot=False, debug=False\n",
        "        )\n",
        "        for method in (methods or summary.keys()):\n",
        "            s = summary.get(method, {})\n",
        "            variance = df_results[df_results[\"method\"] == method]['var_a'].mean() if \"var_a\" in df_results else None\n",
        "            record = {\n",
        "                param_name: param_val,\n",
        "                'mean_f1_assessor': mean_f1,\n",
        "                'mean_prec_assessor': mean_prec,\n",
        "                'mean_rec_assessor': mean_rec,\n",
        "                'mean_fpr_assessor': mean_fpr,\n",
        "                'method': method,\n",
        "            }\n",
        "            for met in metrics:\n",
        "                v = s.get(met)\n",
        "                if v is None and met == \"variance\":\n",
        "                    v = variance\n",
        "                record[met] = v\n",
        "            all_records.append(record)\n",
        "            if debug:\n",
        "                print({**record})\n",
        "    df = pd.DataFrame(all_records)\n",
        "    if plot and not df.empty:\n",
        "        Plotter.plot_f1_vs_param(df, param_name=param_name, methods=df[\"method\"].unique())\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "vNC6q_GA6ZfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOgPK4MuM9oO"
      },
      "outputs": [],
      "source": [
        "def config_fpr():\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 1000,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.03,\n",
        "            \"prevalence\": 0.25,\n",
        "            \"effect_size\": 0.0\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 40,\n",
        "            \"per_task_count\": 12,\n",
        "            \"matrix\": {'TP': 0.8, 'FP': 0.2, 'TN': 0.8, 'FN': 0.2}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def config_power():\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 2000,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.25,\n",
        "            \"effect_size\": 0.15\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 60,\n",
        "            \"per_task_count\": 12,\n",
        "            \"matrix\": {'TP': 0.82, 'FP': 0.18, 'TN': 0.82, 'FN': 0.18}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def config_random_assessors():\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 30,\n",
        "            \"per_task_count\": 7,\n",
        "            \"matrix\": {'TP': 0.5, 'FP': 0.5, 'TN': 0.5, 'FN': 0.5}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def config_informative_assessors():\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 30,\n",
        "            \"per_task_count\": 7,\n",
        "            \"matrix\": {'TP': 1.0, 'FP': 0.0, 'TN': 1.0, 'FN': 0.0}\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def config_high_heterogeneity():\n",
        "    \"\"\"\n",
        "    Большой разброс (хаотичность) между assessors:\n",
        "    каждый имеет индивидуальную матрицу ошибок, с сильным шумом вокруг среднего.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 1000,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.05\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 30,\n",
        "            \"per_task_count\": 7,\n",
        "            \"matrix\": {'TP': 0.7, 'FP': 0.3, 'TN': 0.7, 'FN': 0.3},\n",
        "            \"noise\": 0.3  # БОЛЬШОЙ разнос по толпе!\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def config_ideal(sample_size=200):\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": sample_size,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.22,\n",
        "            \"effect_size\": 0.10\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 40,\n",
        "            \"per_task_count\": 12,\n",
        "            \"matrix\": {'TP': 1.0, 'FP': 0.0, 'TN': 1.0, 'FN': 0.0}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def config_hard():\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 600,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.08,\n",
        "            \"effect_size\": 0.03\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 60,\n",
        "            \"per_task_count\": 10,\n",
        "            \"matrix\": {'TP': 0.7, 'FP': 0.3, 'TN': 0.7, 'FN': 0.3}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def config_sweep(f1_assessor, pr_coeff=4):\n",
        "    tpr, fpr, precision, recall = tpr_fpr_from_f1(f1_assessor, pr_coeff=pr_coeff)\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 300,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.25,\n",
        "            \"effect_size\": 0.18\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 30,\n",
        "            \"per_task_count\": 7,\n",
        "            \"tpr_mean\": tpr,\n",
        "            \"tpr_std\": 0.0,\n",
        "            \"fpr_mean\": fpr,\n",
        "            \"fpr_std\": 0.0,\n",
        "            \"pr_coeff\": pr_coeff   # комментарий коэфф. Precision/Recall, для истории\n",
        "        }\n",
        "    }\n",
        "\n",
        "def config_for_reveal_diff():\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,      # побольше задач — меньше случайного шума\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2       # крупный заметный эффект между группами\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 30,\n",
        "            \"per_task_count\": 3\n",
        "            # В sweep assessors будут перезаписываться функцией generate_diverse_assessors с нужным target_f1 и noise\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def config_ultra_diverse():\n",
        "    np.random.seed()\n",
        "    n_assessors = 30\n",
        "    assessors = []\n",
        "    for i in range(n_assessors):\n",
        "        TP = np.clip(np.random.normal(0.7, 0.3), 0.4, 0.99)\n",
        "        TN = np.clip(np.random.normal(0.7, 0.3), 0.4, 0.99)\n",
        "        FN = 1 - TP\n",
        "        FP = 1 - TN\n",
        "        cmatrix = {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN}\n",
        "        assessors.append(Assessor(i, cmatrix))\n",
        "    return {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": n_assessors,\n",
        "            \"per_task_count\": 7,\n",
        "            \"assessors_list\": assessors\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "EXPERIMENT_CONFIGS = {\n",
        "    'fpr':        config_fpr,\n",
        "    'power':      config_power,\n",
        "    'ideal':      config_ideal,\n",
        "    'hard':       config_hard,\n",
        "}\n",
        "\n",
        "# --- Использование:\n",
        "# config = EXPERIMENT_CONFIGS['fpr']()\n",
        "# df_results, summary = run_exp(config, n_simulations=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AnZ-nhCcrKhf",
        "outputId": "83c98245-8dbe-49ca-efcc-1d97e19cbb36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All validation tests passed!\n"
          ]
        }
      ],
      "source": [
        "def test_power_config():\n",
        "    config = config_power()\n",
        "    df_results, summary = run_exp(config, n_simulations=100)\n",
        "    power = summary['classic']['power']\n",
        "    print(\"[Test Power Config] Power =\", power)\n",
        "    assert power > 0.7, \"Power for good assessors should be high!\"\n",
        "\n",
        "def test_fpr_config():\n",
        "    config = config_fpr()\n",
        "    df_results, summary = run_exp(config, n_simulations=100)\n",
        "    fpr = summary['classic']['fpr']\n",
        "    print(\"[Test FPR Config] FPR =\", fpr)\n",
        "    assert abs(fpr - config['experiment']['alpha']) < 0.04, \"FPR should be near alpha when effect_size=0\"\n",
        "\n",
        "def test_ideal_config():\n",
        "    config = config_ideal(1000)\n",
        "    df_results, summary = run_exp(config, n_simulations=100)\n",
        "    power = summary['classic']['power']\n",
        "    print(\"[Test Ideal Config] Power =\", power)\n",
        "    assert power > 0.9, \"Power for ideal assessors should be close to 1!\"\n",
        "\n",
        "def test_assessor_theory_metrics():\n",
        "    cm = {'TP': 0.8, 'FP': 0.1, 'TN': 0.9, 'FN': 0.2}\n",
        "    a = Assessor(0, cm)\n",
        "    m = a.theory_metrics()\n",
        "    assert abs(m['recall'] - 0.8 / (0.8 + 0.2)) < 1e-6\n",
        "    assert abs(m['fpr'] - 0.1 / (0.9 + 0.1)) < 1e-6\n",
        "    assert 0 <= m['f1'] <= 1\n",
        "    print(\"[Test Assessor Metrics] OK\")\n",
        "\n",
        "\n",
        "def test_probabilistic_aggregation():\n",
        "    cm = {'TP': 0.8, 'FP': 0.1, 'TN': 0.9, 'FN': 0.2}\n",
        "    assessors = [Assessor(i, cm) for i in range(5)]\n",
        "    assessors_by_id = {a.id: a for a in assessors}\n",
        "    # Ответы: пусть 5 человек сказали 1, id=0..4\n",
        "    responses = [(1, i) for i in range(5)]\n",
        "    mean_p, var_p = AssessorProbabilisticAggregator.aggregate_task(responses, assessors_by_id)\n",
        "    assert 0 < mean_p < 1, \"Mean probability after aggregation must be between 0 and 1\"\n",
        "    print(\"[Test Probabilistic Aggregation] OK\")\n",
        "\n",
        "def test_prob_with_random_assessors():\n",
        "    config = config_random_assessors()\n",
        "    df_results, summary = run_exp(config, n_simulations=80)\n",
        "    print(f\"[Random assessors test] Probabilistic F1_test: {summary['probabilistic']['f1_test']}, Classic F1_test: {summary['classic']['f1_test']}\")\n",
        "    diff = abs(summary['probabilistic']['f1_test'] - summary['classic']['f1_test'])\n",
        "    assert diff < 0.15, \"Probabilistic и Classic должны совпадать при неинформативной толпе!\"\n",
        "    assert summary['probabilistic']['f1_test'] < 0.2, \"F1_test должен быть низким при рандомной толпе\"\n",
        "\n",
        "def test_prob_with_informative_assessors():\n",
        "    config = config_informative_assessors()\n",
        "    df_results, summary = run_exp(config, n_simulations=80)\n",
        "    diff = abs(summary['probabilistic']['f1_test'] - summary['classic']['f1_test'])\n",
        "    print(f\"[Informative assessors test] Probabilistic F1_test: {summary['probabilistic']['f1_test']}, Classic F1_test: {summary['classic']['f1_test']}\")\n",
        "    assert diff < 0.1, \"Probabilistic и Classic должны совпадать при идеале!\"\n",
        "    assert summary['probabilistic']['f1_test'] > 0.8, \"F1_test должен быть высоким при идеале!\"\n",
        "\n",
        "\n",
        "\n",
        "def test_ultra_diverse():\n",
        "    config = config_ultra_diverse()\n",
        "    df_results, summary = run_exp(config, n_simulations=80, assessors_override=config['assessors']['assessors_list'])\n",
        "    print(f\"[test_ultra_diverse assessors test] Probabilistic F1_test: {summary['probabilistic']['f1_test']}, Power: {summary['probabilistic']['power']}\")\n",
        "    assert summary['probabilistic']['f1_test'] >= summary['classic']['f1_test'], \"Probabilistic должен быть хотя бы не хуже classic при смешанной толпе\"\n",
        "\n",
        "\n",
        "def test_prob_with_random_assessors():\n",
        "    config = {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2  # даже если заметный эффект\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 30,\n",
        "            \"per_task_count\": 7,\n",
        "            \"matrix\": {'TP': 0.5, 'FP': 0.5, 'TN': 0.5, 'FN': 0.5}\n",
        "        }\n",
        "    }\n",
        "    df_results, summary = run_exp(config, n_simulations=60)\n",
        "    print(f\"[Random assessors test] Probabilistic F1_test: {summary['probabilistic']['f1_test']}, Power: {summary['probabilistic']['power']}\")\n",
        "    assert summary['probabilistic']['f1_test'] < 0.2, \"F1_test должен быть низким при плохих assessors\"\n",
        "    assert abs(summary['probabilistic']['power'] - config[\"experiment\"][\"alpha\"]) < 0.1, \"Power ~ alpha при плохих ассессорах\"\n",
        "\n",
        "def test_probabilistic_pooled_variance():\n",
        "    \"\"\"\n",
        "    Тест: pooled_poibin_variance и метод probabilistic_pooled в VarianceEstimator адекватны:\n",
        "    - variance > 0 для любой физичной толпы assessors\n",
        "    - Power (и F1 теста) хотя бы не ниже classic на хорошем эффекте\n",
        "    - Для гомогенной толпы все методы должны совпадать по дисперсии и F1\n",
        "    \"\"\"\n",
        "\n",
        "    # Сгенерируем assessors средней силы с умеренной гетерогенностью:\n",
        "    assessors, _ = Assessor.generate_diverse_assessors(\n",
        "        n=30, target_f1=0.7, pr_coeff=1, noise_std=0.01\n",
        "    )\n",
        "\n",
        "    config = {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": len(assessors),\n",
        "            \"per_task_count\": 7\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Подключаем pooled-метод как отдельный ключ для run_exp:\n",
        "    methods = {\n",
        "        \"classic\": VarianceEstimator.classic,\n",
        "        \"probabilistic\": VarianceEstimator.probabilistic,\n",
        "        \"probabilistic_pooled\": VarianceEstimator.probabilistic_pooled,\n",
        "        \"probabilistic_pooled_taskwise\": VarianceEstimator.probabilistic_pooled_taskwise\n",
        "    }\n",
        "\n",
        "    df_results, summary = run_exp(config, n_simulations=60, methods=methods, assessors_override=assessors)\n",
        "    print(f\"\\n[pooled variance test] classic F1_test: {summary['classic']['f1_test']:.3f}\")\n",
        "    print(f\"[pooled variance test] probabilistic F1_test: {summary['probabilistic']['f1_test']:.3f}\")\n",
        "    print(f\"[pooled variance test] probabilistic_pooled F1_test: {summary['probabilistic_pooled']['f1_test']:.3f}\")\n",
        "\n",
        "    # Проверяем базовые sanity -- pooled-variance встроен и не обнуляется\n",
        "    var_a = df_results[df_results['method'] == 'probabilistic_pooled']['var_a'].mean()\n",
        "    var_b = df_results[df_results['method'] == 'probabilistic_pooled']['var_b'].mean()\n",
        "    assert var_a > 0 and var_b > 0, \"Pooled variance должен быть положителен!\"\n",
        "\n",
        "    # В гомогенной крутой толпе методы практически совпадают\n",
        "    assessors_homog, _ = Assessor.generate_diverse_assessors(30, target_f1=0.9, pr_coeff=1, noise_std=0.0)\n",
        "    config[\"assessors\"][\"total_count\"] = len(assessors_homog)\n",
        "    df_results_h, summary_h = run_exp(config, n_simulations=30, methods=methods, assessors_override=assessors_homog)\n",
        "    f1s = [summary_h[m]['f1_test'] for m in methods]\n",
        "\n",
        "\n",
        "    print(f1s)\n",
        "\n",
        "def test_probabilistic_pooled_variance_with_density(debug=False):\n",
        "    assessors = Assessor.generate_diverse_assessors(20, target_f1=0.7, pr_coeff=1, noise_std=0.08)\n",
        "    config = {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 400,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.2\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": len(assessors),\n",
        "            \"per_task_count\": 7\n",
        "        }\n",
        "    }\n",
        "    methods = {\n",
        "        \"classic\": VarianceEstimator.classic,\n",
        "        \"probabilistic\": VarianceEstimator.probabilistic,\n",
        "        \"probabilistic_pooled\": VarianceEstimator.probabilistic_pooled,\n",
        "        \"probabilistic_pooled_taskwise\": VarianceEstimator.probabilistic_pooled_taskwise\n",
        "    }\n",
        "\n",
        "    df_results, summary = run_exp(config, n_simulations=40, methods=methods, assessors_override=assessors, plot=True, debug=debug)\n",
        "    print(\"\\n[pooled variance test] F1_test:\", {m: summary[m]['f1_test'] for m in summary})\n",
        "\n",
        "    n_experiments = 500\n",
        "    n_tasks_per_exp = 40\n",
        "    frac_zeros = simulate_fraction_zeros_mv(assessors, prevalence=0.5, n_experiments=n_experiments, n_tasks_per_exp=n_tasks_per_exp)\n",
        "    q = theoretical_prob_majority_zero(assessors, prevalence=0.5)\n",
        "    Plotter.plot_binomial_vs_hist(frac_zeros, n_tasks_per_exp, q, title=\"Pooled Variance: MV fraction zeros\")\n",
        "\n",
        "    print(f\"Empirical mean: {frac_zeros.mean():.4f}, std: {frac_zeros.std():.5f}\")\n",
        "    print(f\"Theoretical mean: {q:.4f}, std: {np.sqrt(q * (1-q)/n_tasks_per_exp):.5f}\")\n",
        "\n",
        "def test_mean_and_majority_empirical_theory(\n",
        "    n_assessors=20, target_f1=0.7, pr_coeff=1, noise_std=0.08,\n",
        "    true_label=1, n_experiments=500, n_trials_per_exp=50, debug=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Тестирует построение распределения mean-агрегации и majority-агрегации\n",
        "    и сравнивает с теоретической кривой (по Central Limit).\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from scipy.stats import norm\n",
        "\n",
        "    def poisson_binomial_pmf(ps):\n",
        "        N = len(ps)\n",
        "        pmf = np.zeros(N + 1)\n",
        "        pmf[0] = 1.0\n",
        "        for p in ps:\n",
        "            pmf[1:] = pmf[1:] * (1 - p) + pmf[:-1] * p\n",
        "            pmf[0] *= (1 - p)\n",
        "        return pmf\n",
        "\n",
        "    assessors, emp_f1 = Assessor.generate_diverse_assessors(\n",
        "        n_assessors, target_f1=target_f1, pr_coeff=pr_coeff, noise_std=noise_std)\n",
        "    print(assessors)\n",
        "    N = len(assessors)\n",
        "    ps = []\n",
        "    for a in assessors:\n",
        "        m = a.theory_metrics()\n",
        "        if true_label == 1:\n",
        "            p = m['recall']\n",
        "        else:\n",
        "            p = m['fpr']\n",
        "        ps.append(p)\n",
        "    ps = np.array(ps)\n",
        "\n",
        "    experiment_means, experiment_majority = [], []\n",
        "    for _ in range(n_experiments):\n",
        "        mean_agg, maj_agg = [], []\n",
        "        for _ in range(n_trials_per_exp):\n",
        "            responses = np.array([a.rate(true_label) for a in assessors])\n",
        "            mean_agg.append(np.mean(responses))\n",
        "            maj_agg.append(int(np.sum(responses) >= (N // 2 + 1)))\n",
        "        experiment_means.append(np.mean(mean_agg))\n",
        "        experiment_majority.append(np.mean(maj_agg))\n",
        "    experiment_means = np.array(experiment_means)\n",
        "    experiment_majority = np.array(experiment_majority)\n",
        "    emp_majority_vote_rate = np.mean(experiment_majority >= 0.5)\n",
        "\n",
        "    # Теоретика для mean\n",
        "    mu = np.mean(ps)\n",
        "    var_vote = np.sum(ps*(1-ps))/N**2\n",
        "    theory_std = np.sqrt(var_vote / n_trials_per_exp)\n",
        "    x = np.linspace(min(experiment_means.min(), experiment_majority.min()),\n",
        "                    max(experiment_means.max(), experiment_majority.max()), 250)\n",
        "    gaussian_curve = norm.pdf(x, mu, theory_std)\n",
        "\n",
        "    # Теоретика для majority==1\n",
        "    pmf = poisson_binomial_pmf(ps)\n",
        "    majority_threshold = N // 2 + 1\n",
        "    theory_majority_vote_prob = pmf[majority_threshold:].sum()\n",
        "\n",
        "    # Визуализация\n",
        "    plt.figure(figsize=(9,5))\n",
        "    plt.hist(experiment_means, bins=20, density=True, alpha=0.6, label='Empirical mean agg')\n",
        "    plt.plot(x, gaussian_curve, 'r-', lw=2, label='Theoretical Mean (Normal)')\n",
        "    plt.hist(experiment_majority, bins=20, density=True, alpha=0.4, label='Empirical majority mean')\n",
        "    plt.axvline(emp_majority_vote_rate, color='orange', linestyle='--', lw=2, label='Empirical Majority=1 rate')\n",
        "    plt.axvline(theory_majority_vote_prob, color='brown', linestyle=':', lw=2, label='Theoretical Majority=1 prob')\n",
        "    plt.xlabel('Mean vote (fraction of \"1\"-votes\")')\n",
        "    plt.ylabel('Density / Probability')\n",
        "    plt.title('Mean & Majority vote: Empirical vs Theoretical')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Empirical mean (mean agg): {experiment_means.mean():.4f}, std: {experiment_means.std():.5f}\")\n",
        "        print(f\"Empirical mean (majority): {experiment_majority.mean():.4f}, std: {experiment_majority.std():.5f}\")\n",
        "        print(f\"Empirical majority==1 rate: {emp_majority_vote_rate:.4f}\")\n",
        "        print(f\"Theoretical mean: {mu:.4f}, std: {theory_std:.5f}\")\n",
        "        print(f\"Theoretical P(majority_vote==1): {theory_majority_vote_prob:.4f}\")\n",
        "\n",
        "    # Автоматическая простая проверка shape/center\n",
        "    assert abs(experiment_means.mean() - mu) < 0.05, \"Mean-mean агрегации: теоретика и эмпирика не совпадают\"\n",
        "    assert abs(emp_majority_vote_rate - theory_majority_vote_prob) < 0.06, \"Majority==1 rate: теория/эксперимент не совпали\"\n",
        "    print(\"[OK] Тест Mean & Majority aggregation vs теория прошёл\")\n",
        "\n",
        "def test_pvalue_calibration_null(debug=False):\n",
        "    config = {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": 100,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": 0.0,\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": 200,\n",
        "            \"per_task_count\": 3,\n",
        "            \"target_f1\": 0.55,\n",
        "            \"pr_coeff\": 1,\n",
        "            \"noise\": 0.3\n",
        "        }\n",
        "    }\n",
        "\n",
        "    methods = {\n",
        "        \"classic\": VarianceEstimator.classic,\n",
        "        \"probabilistic\": VarianceEstimator.probabilistic,\n",
        "        \"probabilistic_pooled\": VarianceEstimator.probabilistic_pooled,\n",
        "        \"probabilistic_pooled_taskwise\": VarianceEstimator.probabilistic_pooled_taskwise\n",
        "    }\n",
        "    # Запускаем с большим n_simulations\n",
        "    df_results, summary = run_exp(config, n_simulations=1000, methods=methods, plot=False, debug=debug)\n",
        "\n",
        "    # Гистограммы p-value\n",
        "    methods_list = list(methods.keys())\n",
        "    plt.figure(figsize=(10,5))\n",
        "    for method in methods_list:\n",
        "        pvals = df_results[df_results['method']==method]['p_value'].dropna().values\n",
        "        plt.hist(pvals, bins=20, alpha=0.4, density=True, label=method.capitalize())\n",
        "    plt.axhline(1, color='black', linestyle='--', label='Uniform (идеал)')\n",
        "    plt.xlabel('p-value')\n",
        "    plt.ylabel('Плотность')\n",
        "    plt.title(\"p-value calibration (Null hypothesis, effect_size=0)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Fraction(p < alpha)\n",
        "    alpha = config[\"experiment\"][\"alpha\"]\n",
        "    print(f'\\nFraction p-value < alpha={alpha}:')\n",
        "    for method in methods_list:\n",
        "        pvals = df_results[df_results['method']==method]['p_value'].dropna().values\n",
        "        frac = np.mean(pvals < alpha)\n",
        "        print(f\"{method}: {frac:.3f} (ожидается ~{alpha:.2f})\")\n",
        "\n",
        "\n",
        "def test_generate_diverse_assessors():\n",
        "    \"\"\"\n",
        "    Тест-кейс для Assessor.generate_diverse_assessors:\n",
        "    - Проверяет: всегда ли возвращается плоский список assessors, нужного размера.\n",
        "    - Не возвращает пустых или странных ассессоров.\n",
        "    - Показывает средний f1, разброс, диапазон значений.\n",
        "    - Скипает (и выводит предупреждение) если не удалось сгенерировать достаточное число ассессоров.\n",
        "    \"\"\"\n",
        "    n = 20\n",
        "    target_f1 = 0.6\n",
        "    pr_coeff = 1\n",
        "    noise = 0.05\n",
        "\n",
        "    assessors = Assessor.generate_diverse_assessors(n, target_f1=target_f1, pr_coeff=pr_coeff, noise_std=noise)\n",
        "    if not isinstance(assessors, list) or not all(isinstance(a, Assessor) for a in assessors):\n",
        "        raise AssertionError(\"generate_diverse_assessors должен возвращать список объектов Assessor!\")\n",
        "    if len(assessors) < n:\n",
        "        print(f\"[SKIP TEST] Only {len(assessors)} of {n} assessors generated at F1={target_f1}, pr_coeff={pr_coeff}, noise={noise}\")\n",
        "        return\n",
        "    all_cm = [tuple(sorted(a.confusion_matrix.items())) for a in assessors]\n",
        "    n_unique = len(set(all_cm))\n",
        "    f1s = [a.theory_metrics()['f1'] for a in assessors]\n",
        "    pre = [a.theory_metrics()['precision'] for a in assessors]\n",
        "    rec = [a.theory_metrics()['recall'] for a in assessors]\n",
        "    fpr = [a.theory_metrics()['fpr'] for a in assessors]\n",
        "    for x in f1s + pre + rec + fpr:\n",
        "        assert 0 < x < 1, f\"Некорректное значение метрики: {x}\"\n",
        "    print(f\"[OK] generate_diverse_assessors: играющих ассессоров={len(assessors)}, \"\n",
        "          f\"mean_f1: {np.mean(f1s):.3f}, мин/макс f1: {np.min(f1s):.3f}/{np.max(f1s):.3f}, \"\n",
        "          f\"precision: {np.mean(pre):.3f}, recall: {np.mean(rec):.3f}, fpr: {np.mean(fpr):.3f}, \"\n",
        "          f\"уникальных матриц: {n_unique}/{n}\")\n",
        "\n",
        "\n",
        "\n",
        "def test_pooled_variance_lognics(debug=False):\n",
        "    \"\"\"\n",
        "    Тест декомпозиции pooled variance:\n",
        "    - Сравнить pooled по формуле vs MC-симуляцию mean/var single task.\n",
        "    - Сравнить среднюю по группе задач, убедиться, что суммы соотносятся корректно.\n",
        "    \"\"\"\n",
        "    n_assessors = 10\n",
        "    n_tasks = 20\n",
        "    n_trials = 5000  # MC for each task\n",
        "\n",
        "    # Сгенерируем толпу assessors с разным quality близким к .7\n",
        "    assessors = Assessor.generate_diverse_assessors(n_assessors, target_f1=0.7, pr_coeff=1, noise_std=0.06)\n",
        "    # Повторим для набора задач fixed true_label\n",
        "\n",
        "    empirical_vars = []\n",
        "    theoretical_vars = []\n",
        "    for _ in range(n_tasks):\n",
        "        # Для одной задачи считаем ответы по всем assessors много раз (MC)\n",
        "        true_label = np.random.choice([0, 1])\n",
        "        ps = []\n",
        "        for a in assessors:\n",
        "            c = a.confusion_matrix\n",
        "            if true_label == 1:\n",
        "                ps.append(c['TP'] / (c['TP'] + c['FN']))\n",
        "            else:\n",
        "                ps.append(c['FP'] / (c['FP'] + c['TN']))\n",
        "        MC_means = []\n",
        "        for _ in range(n_trials):\n",
        "            votes = [np.random.rand() < p for p in ps]\n",
        "            MC_means.append(np.mean(votes))\n",
        "        emp_var = np.var(MC_means)\n",
        "        empirical_vars.append(emp_var)\n",
        "        # Теперь аналитическая формула\n",
        "        theo_var = np.sum([p*(1-p) for p in ps]) / n_assessors**2\n",
        "        theoretical_vars.append(theo_var)\n",
        "        if debug:\n",
        "            print(f\"[Task] label={true_label}, Empirical MC variance={emp_var:.6f}, Theo.={theo_var:.6f}\")\n",
        "    # Проверим среднее отличие\n",
        "    emp_mean = np.mean(empirical_vars)\n",
        "    theo_mean = np.mean(theoretical_vars)\n",
        "    print(f\"\\nСредняя variance (MC): {emp_mean:.6f}, (Theory): {theo_mean:.6f}, разн.={abs(emp_mean - theo_mean):.2e}\")\n",
        "    assert abs(emp_mean - theo_mean) < 2e-3, \"Разница между эмпирической и теоретической variance должна быть мала!\"\n",
        "    print(\"[OK] Pooled variance taskwise совпадает с симуляцией для средней толпы!\")\n",
        "\n",
        "\n",
        "\n",
        "def run_all_validation_tests():\n",
        "    # test_generate_diverse_assessors()\n",
        "    # test_assessor_theory_metrics()\n",
        "    # test_probabilistic_aggregation()\n",
        "    # test_power_config()\n",
        "    # test_fpr_config()\n",
        "    # test_ideal_config()\n",
        "    # test_prob_with_random_assessors()\n",
        "    # test_pooled_variance_lognics()\n",
        "    # test_ultra_diverse()\n",
        "    # test_probabilistic_pooled_variance_with_density(debug=True)\n",
        "    # test_probabilistic_pooled_variance()\n",
        "    # test_pvalue_calibration_null()\n",
        "    # test_mean_and_majority_empirical_theory()\n",
        "    print(\"All validation tests passed!\")\n",
        "\n",
        "run_all_validation_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmqnrAzZMPnV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uIlgXMllMiwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LauncPad\n"
      ],
      "metadata": {
        "id": "aiU-D-ezACfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# --- SAVE FUNCTION ---\n",
        "def save_experiment_case(config, df, summary, base_dir=\"ab_experiments\", tag=None, exp_name=None):\n",
        "    now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    save_dir = os.path.join(base_dir, f\"exp_{now}\" + (f\"_{tag}\" if tag else \"\") + (f\"_{exp_name}\" if exp_name else \"\"))\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf8\") as f:\n",
        "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "    df.to_csv(os.path.join(save_dir, \"results.csv\"), index=False)\n",
        "    with open(os.path.join(save_dir, \"summary.json\"), \"w\", encoding=\"utf8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Эксперимент сохранён в: {save_dir}\")\n",
        "    return save_dir\n",
        "\n",
        "# --- Виджеты (см. твою логику выше) ---\n",
        "f1_slider = widgets.FloatSlider(min=0.2, max=0.95, step=0.02, value=0.7, description='F1:')\n",
        "noise_slider = widgets.FloatSlider(min=0.0, max=0.25, step=0.01, value=0.04, description='Noise:')\n",
        "n_assessors_slider = widgets.IntSlider(min=5, max=50, value=20, description='Assessors:')\n",
        "pr_coeff_slider = widgets.FloatSlider(min=0.5, max=3.0, value=1.0, description='pr_coeff:')\n",
        "coverage_slider = widgets.IntSlider(min=1, max=20, value=5, description='Assessors/task:')\n",
        "sample_size_slider = widgets.IntSlider(min=50, max=400, step=10, value=120, description='Sample:')\n",
        "n_simulations_slider = widgets.IntSlider(min=10, max=200, step=10, value=30, description='Sim:')\n",
        "effect_slider = widgets.FloatSlider(min=0.0, max=0.5, value=0.2, step=0.01, description='Effect:')\n",
        "exp_name_text = widgets.Text(value=\"ab_experiment\", description='Название:')\n",
        "\n",
        "metric_options = [\n",
        "    (\"F1-score\"     , \"f1_test\"),\n",
        "    (\"Power\"        , \"power\"),\n",
        "    (\"FPR\"          , \"fpr\"),\n",
        "    (\"Mean Effect\"  , \"mean_effect\"),\n",
        "    (\"Variance\"     , \"variance\"),\n",
        "]\n",
        "metric_selector = widgets.SelectMultiple(\n",
        "    options=metric_options,\n",
        "    value=('f1_test', 'variance'),\n",
        "    description=\"Metrics\"\n",
        ")\n",
        "method_selector = widgets.SelectMultiple(\n",
        "    options=[\n",
        "        ('classic', 'classic'),\n",
        "        ('probabilistic', 'probabilistic'),\n",
        "        ('probabilistic_pooled', 'probabilistic_pooled'),\n",
        "        ('probabilistic_pooled_taskwise', 'probabilistic_pooled_taskwise')\n",
        "    ],\n",
        "    value=('classic',\n",
        "           'probabilistic',\n",
        "           'probabilistic_pooled',\n",
        "           'probabilistic_pooled_taskwise'),\n",
        "    description=\"Methods\"\n",
        ")\n",
        "sweep_param_selector = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('F1 assessors', 'f1'),\n",
        "        ('Noise', 'noise'),\n",
        "        ('Assessors per task', 'per_task_count')\n",
        "    ],\n",
        "    value='f1', description='Sweep param:'\n",
        ")\n",
        "run_button = widgets.Button(description=\"Run sweep!\", button_style='primary')\n",
        "save_button = widgets.Button(description=\"Сохранить эксперимент\", button_style='success', disabled=True)\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- local state ---\n",
        "last_df, last_summary, last_config = None, None, None\n",
        "\n",
        "def run_ipynb_sweep(\n",
        "    f1=0.7, noise=0.04, n_assessors=20, pr_coeff=1.0, per_task_count=5,\n",
        "    sample_size=120, n_simulations=30, effect=0.2, methods=('classic', 'probabilistic'),\n",
        "    sweep_param='f1'\n",
        "):\n",
        "    param = sweep_param\n",
        "    points = 10\n",
        "    if param == 'f1':\n",
        "        param_grid = [(v, {'target_f1': v, 'pr_coeff': pr_coeff, 'noise_std': noise, 'per_task_count': per_task_count})\n",
        "                      for v in np.linspace(0.2, 0.9, points)]\n",
        "    elif param == 'noise':\n",
        "        param_grid = [(v, {'target_f1': f1, 'pr_coeff': pr_coeff, 'noise_std': v, 'per_task_count': per_task_count})\n",
        "                      for v in np.linspace(0.01, 0.25, points)]\n",
        "    elif param == 'per_task_count':\n",
        "        param_grid = [(int(v), {'target_f1': f1, 'pr_coeff': pr_coeff, 'noise_std': noise, 'per_task_count': int(v)})\n",
        "                      for v in np.linspace(1, min(n_assessors, 20), points)]\n",
        "    else:\n",
        "        raise ValueError(param)\n",
        "    base_config = {\n",
        "        \"experiment\": {\n",
        "            \"sample_size\": sample_size,\n",
        "            \"groups\": [\"A\", \"B\"],\n",
        "            \"alpha\": 0.05,\n",
        "            \"prevalence\": 0.5,\n",
        "            \"effect_size\": effect\n",
        "        },\n",
        "        \"assessors\": {\n",
        "            \"total_count\": n_assessors,\n",
        "            \"per_task_count\": per_task_count\n",
        "        }\n",
        "    }\n",
        "    methods_dict = {m: getattr(VarianceEstimator, m) for m in methods if hasattr(VarianceEstimator, m)}\n",
        "    df = universal_sweep(\n",
        "        param_grid,\n",
        "        param_name=param,\n",
        "        sweep_type='f1',\n",
        "        base_config=base_config,\n",
        "        n_assessors=n_assessors,\n",
        "        n_simulations=n_simulations,\n",
        "        plot=False,\n",
        "        methods=methods_dict,\n",
        "        debug=False\n",
        "    )\n",
        "    return df, param, base_config\n",
        "\n",
        "def on_run_clicked(b):\n",
        "    global last_df, last_summary, last_config\n",
        "    output.clear_output()\n",
        "    with output:\n",
        "        df, param, base_config = run_ipynb_sweep(\n",
        "            f1=f1_slider.value,\n",
        "            noise=noise_slider.value,\n",
        "            n_assessors=n_assessors_slider.value,\n",
        "            pr_coeff=pr_coeff_slider.value,\n",
        "            per_task_count=coverage_slider.value,\n",
        "            sample_size=sample_size_slider.value,\n",
        "            n_simulations=n_simulations_slider.value,\n",
        "            effect=effect_slider.value,\n",
        "            methods=method_selector.value,\n",
        "            sweep_param=sweep_param_selector.value\n",
        "        )\n",
        "        if df.empty:\n",
        "            print(\"Не удалось сгенерировать ни одной валидной точки!\")\n",
        "            save_button.disabled = True\n",
        "            return\n",
        "        metrics_to_plot = tuple(metric_selector.value)\n",
        "        fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(9, 5 * len(metrics_to_plot)))\n",
        "        if len(metrics_to_plot) == 1: axes = [axes]\n",
        "        for idx, metric in enumerate(metrics_to_plot):\n",
        "            ax = axes[idx]\n",
        "            for method in df['method'].unique():\n",
        "                d = df[df['method'] == method]\n",
        "                ax.plot(d[param], d[metric], '-o', label=method.capitalize())\n",
        "            ax.set_xlabel(param)\n",
        "            ax.set_ylabel(metric)\n",
        "            ax.set_title(f\"{metric} по {param}\")\n",
        "            ax.legend()\n",
        "            ax.grid()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        summary = {}\n",
        "        for m in df['method'].unique():\n",
        "            d = df[df['method'] == m]\n",
        "            summary[m] = {metric: float(np.nanmean(d[metric])) for _, metric in metric_options}\n",
        "        last_df = df.copy()\n",
        "        last_summary = summary.copy()\n",
        "        last_config = base_config.copy()\n",
        "        save_button.disabled = False\n",
        "        display(df)\n",
        "        # display(widgets_ui)\n",
        "\n",
        "def on_save_clicked(b):\n",
        "    if last_df is not None and last_summary is not None and last_config is not None:\n",
        "        save_experiment_case(last_config, last_df, last_summary, tag=\"launchpad\", exp_name=exp_name_text.value)\n",
        "    else:\n",
        "        print(\"Нет данных для сохранения!\")\n",
        "\n",
        "run_button.on_click(on_run_clicked)\n",
        "save_button.on_click(on_save_clicked)\n",
        "\n",
        "widgets_ui = widgets.VBox([\n",
        "    widgets.HBox([f1_slider, noise_slider]),\n",
        "    widgets.HBox([n_assessors_slider, pr_coeff_slider]),\n",
        "    widgets.HBox([coverage_slider, sample_size_slider, n_simulations_slider, effect_slider]),\n",
        "    method_selector, metric_selector, sweep_param_selector, exp_name_text,\n",
        "    widgets.HBox([run_button, save_button]), output\n",
        "])"
      ],
      "metadata": {
        "id": "3U4ZN13sniJl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "XhyffHd0AhGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display(widgets_ui)"
      ],
      "metadata": {
        "id": "2dgDOjlZicpP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyprIYob3wIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}